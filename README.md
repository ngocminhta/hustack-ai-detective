# HUSTack-AI Detective Documentation

HUSTack-AI Detective is an API built with FastAPI and Gradio designed to analyze source code and determine whether it is human-written or AI-generated. In advanced mode, it can also identify the AI model family that generated the code. The API leverages Hugging Face’s Transformers pipelines to perform text classification on the provided code after cleaning and preprocessing it.

## Features

- **AI vs. Human Detection:** Uses a text classification model to decide if the submitted code is human-written or generated by AI.
- **Advanced Model Detection:** In advanced mode, if the code is flagged as AI-generated, an additional model identifies the family of AI models (e.g., Gemini 1.x Family, GPT Family).
- **Interactive Web Interface:** Built using Gradio, enabling users to interactively test and analyze code snippets.

## Prerequisites

This project is distributed as a Docker image. You will need Docker installed on your system to run the application.

- **Docker:** Install Docker on your machine. Follow the instructions at the [Docker documentation](https://docs.docker.com/get-docker/) for your specific operating system.
- **Docker image:**
```sh
docker pull ngocminhta/hustack-ai-detective:latest
docker run -e PORT=8000 ngocminhta/hustack-ai-detective:latest
```
- **Model Files:** The Docker image includes the required dependencies. However, ensure that the directories `./ai-detector` and `./model-detector` (mounted appropriately) contain the necessary text-classification models (compatible with Hugging Face Transformers).

## Installation

**Step 1. Clone the Repository:**

```sh
git clone <repository-url>
cd <repository-directory>
```

**Step 2. Build the Docker Image:**

```sh
docker build -t hustack-ai-detective .
```

**Step 3. Running the Docker Container:**

```sh
docker run -p 8000:8000 hustack-ai-detective
```

   The API will be accessible at `http://localhost:8000`.

## API Endpoints

### POST `/classify`

- **Description:**  
  Classifies the provided source code to determine its origin and optionally identifies the AI model family if in advanced mode.

- **Request Body (JSON):**

```json
{
"code": "your code here",
"language": "Python",  // Options: C, C++, Java, Python
"mode": "normal"       // or "advanced"
}
```

- **Response:**
  - **Normal Mode:** Returns the source type:

```json
{
  "source": "Human"  // or "AI"
}
```

  - **Advanced Mode:** If the code is detected as AI-generated, returns the AI model family:

```json
{
  "source": "AI",
  "ai_model": "GPT Family"  // as detected by the model
}
```

## Interactive Interface

The project integrates a Gradio interface for testing the classification system interactively. Also, user can read the instruction at README page ([/readme](/readme)):

- **Interface Features:**
  - **Language Selection:** Choose between C, C++, Java, and Python.
  - **Code Input:** Enter or paste the source code into the provided code box.
  - **"Check Origin" Button:** Triggers the classification process.
  - **"Clear" Button:** Resets the inputs.
  - **Results Display:** Shows the classification result and, in advanced mode, detailed model information.

When you run the application via Docker, the Gradio UI is mounted and available at the root path of the API server.

-----------
© This product is developed by [Minh N. Ta](https://tnminh.com) and contributed to the School of Information and Communication Technology, Hanoi University of Science and Technology in 2025.